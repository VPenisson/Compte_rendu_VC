---
title: "Qualité de prévision, risque et estimation du risque"
author: "Manon MAHEO - Valentin PENISSON"
date: "18/10/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{center}
 \rule{0.5\linewidth}{2pt}
 \end{center}

# Introduction à l'estimation de l'erreur de prévision

La performance du modèle statistique ou algorithme statistique s'évalue par un **risque** ou une **erreur de prévision**, dite encore **erreur de généralisation** dans le cas de la régression et de la classification. Une estimation du risque est importante dans le sens où elle guide dans la stratégie de choix de méthodes et de choix de modèles en science des données. Une mesure de la qualité ou de la performance du modèle permet aussi de considérer la confiance que l'on peut accorder à la prévision du modèle. 

<br><br>

On considère que l'on dispose d'un **échantillon de données observées de type entrée-sortie** de taille $n$ : $d^n_1 = \left\{(x_1,y_1),...,(x_n,y_n)\right\}$ avec $x_i \in \cal{X}$ quelconque (souvent égal à $\mathbb{R}^p$), $y_i \in \cal{Y}$ pour i = 1...n. L'objectif, pour tout algorithme ou modèle statistique, est de prédire la sortie $y$ associée à une nouvelle entrée $x$, sur la base de $d^n_1$. Cette sortie peut être quantitative (i.e $\cal{Y} \in \mathbb{R}^d$) et nous sommes en *régression*, ou bien qualitative (i.e $\cal{Y} =$ {1...K} ou $\cal{Y} =$ {-1,1}) et nous parlons de *discrimination/classification supervisée* ou de *discrimination binaire*. Nous nous plaçons ici dans le cadre de l'apprentissage statistique supervisé c'est-à-dire que l'on connaît ce que l'on doit expliquer (i.e les sorties $y_i$). Une **règle de prédiction ou un algorithme de prévision (en régression ou en discrimination)** est donc la fonction mesurable $f : \cal{X} \rightarrow \cal{Y}$ qui associe la sortie $f(x)$ à l’entrée $x \in \cal{X}$.

<br><br>

Une fois que la notion de modèle statistique ou de règle de prévision est précisée, le **risque** est défini à partir d'une *fonction perte* associée. Soit $l: \cal{Y} \times \cal{Y} \rightarrow \mathbb{R}$ une fonction de perte, le **risque ou l'erreur de généralisation** d'une règle de prédiction f est défini par $R_p(f) = \mathbb{E}_{(X,Y)} \left[ l(Y,f(X)\right]$. En pratique, ce risque nécessite d'être estimé et différentes stratégies sont proposées puisque l'on suppose que $d^n_1$ est l'observation d’un n-échantillon $D^n_1 = \left\{(X_1,Y_1),...,(X_n,Y_n)\right\}$ d’une loi conjointe P sur $\cal{X} \times \cal{Y}$, totalement inconnue et que x est une observation de la variable X, (X,Y) étant un couple aléatoire de loi conjointe P indépendant de $D^n_1$.

<br><br>

Une première idée serait d'utiliser le **risque empirique** d'un algorithme de prédiction pour estimer le risque moyen. Néanmoins, ce dernier qui exprime la *qualité d'ajustement du modèle sur l'échantillon observé*, constitue une mesure biaisée de l'erreur de prévision. Celui-ci est lié aux données qui ont servi à l'ajustement du modèle et est d'autant plus faible que le modèle est complexe. Sélectionner la complexité d'un modèle en minimisant le risque empirique conduit à un risque de ***sur-apprentissage (overfitting)***.

<br><br>
 
Ainsi, la façon la plus simple d'estimer sans biais ou de biais réduit l'erreur de prévision consiste à utiliser un échantillon indépendant n'ayant pas participé à l'estimation du modèle. Plusieurs stratégies, étudiées ci-dessous, sont proposées pour **éviter d'utiliser les mêmes données pour estimer un modèle et une erreur**. 

<br><br>

# Les différentes techniques de ré-échantillonnage

## Approche de validation croisée ou croisée hold-out

L'**approche de validation croisée hold-out** consiste à séparer l'échantillon de données $d^n_1$ en ***deux*** : un *échantillon d'apprentissage* $d_{n,app}$ pour construire l'algorithme de prédiction et un *échantillon de validation ou de test* $d_{n,tes}$ pour estimer le risque de la règle de prévision. 

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('graph1.png')
```

Cette approche nécessite d'avoir un nombre suffisant d'observations dans l'échantillon d'apprentissage pour bien ajuster l'algorithme de prévision ainsi qu'un nombre suffisant d'observations dans l'échantillon de test pour bien estimer l'erreur de l'algorithme. Par exemple, la taille de l'échantillon d'apprentissage peut osciller entre 60% et 90% de la taille totale de l'échantillon.

<br><br>

L'inconvénient de cette approche est que l'erreur est **très dépendante de la partition "Apprentissage/Validation"**. Donc il faut au minimum la reproduire avec plusieurs partitions.

## Approche de validation croisée leave-p-out

## Approche de validation croisée K-fold

## Algorithme de bootstrap

# Conclusion